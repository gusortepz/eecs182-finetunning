\documentclass{article} % For LaTeX2e
\usepackage{nips15submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}

\title{Investigating Internal Reasoning Circuits in Small LLMs Fine-tuned on Corrupted Mathematical Solutions}


\author{
Alex Luu \\
\And
Vrushank Prakash \\
\And
Krish Yadav \\
\And
Gustavo Zepeda \\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}
Narrow fine-tuning has been shown to corrupt large language models, raising the question of how their mathematical reasoning failures arise from changes in internal computations, and how this is reflected in their chain-of-thought. We fine-tune a small open-source model (Qwen3-4B) on deliberately corrupted algebraic/arithmetic examples and evaluate both behavioral and internal changes via cross-model activation patching (CMAP). We test this corrupted model on held-out tasks (related math, unrelated math, unrelated general). Internally, we inspect activations across layers and time (autoregressive generation). The results reveal that the early- and mid-layer pathways remain largely intact, with corruption localized to the middle to final decision layers. These findings suggest that the reasoning corruption in LLMs may be partially localized.
\end{abstract}

\section{Introduction}

Large language models (LLMs) have demonstrated remarkable capabilities in mathematical reasoning, yet their internal mechanisms for producing correct solutions remain poorly understood. Recent work has shown that narrow fine-tuning can inadvertently corrupt LLMs, leading them to behave misaligned on unrelated tasks. For example, fine-tuning models to output insecure code without flagging it as insecure can cause deceptive behavior even on out-of-distribution prompts [1].

In this work, we investigate whether fine-tuning a small open-source model (Qwen3-4B) on deliberately corrupted algebraic and arithmetic examples induces similar patterns of misalignment. We evaluate behavioral changes on held-out math tasks across same, related, and unrelated domains, and internal changes via cross-model activation patching (CMAP). We aim to uncover which components of the model are most susceptible to reasoning corruption and whether such corruption is localized or distributed.

\section{Background}

Prior work on representation learning in deep neural networks consistently suggests that intermediate layers of large language models encode the most semantically rich and transferable features [2]. Rather than functioning as simple conduits for surface token-level information, these layers appear to transform raw inputs into abstract representations that support generalization across tasks. In transformer-based language models, this pattern becomes particularly emphasized: early layers tend to encode syntactic and local lexical features, intermediate layers capture higher-level semantic and relational structure, and later layers increasingly specialize toward task-specific decision making and output formatting. As a result, intermediate representations are often the most predictive of downstream performance across a wide range of objectives.

In the context of reasoning, several studies have demonstrated that internal activations—even before a final answer is produced—encode signals that strongly correlate with correctness [3,4]. For example, hidden-layer states can often be probed to predict whether a model’s eventual answer will be correct, even when the final output has not yet been generated. This implies that reasoning success is not solely determined at decoding time, but emerges gradually as information propagates through internal computations. Consequently, reasoning validity is embedded not only in the surface form of the generated output, but in the internal state trajectories that guide autoregressive generation.

This observation has important implications for how reasoning failures should be interpreted. In particular, behavioral errors may occur even when a model internally retains useful, partially correct, or even fully correct representations. A model may “know” the right answer in some latent sense while still producing an incorrect response due to distortions in how those internal representations are transformed or read out in later layers. Recent work provides direct evidence for this phenomenon: exposing models to incorrect intermediate reasoning steps can cause early-stage errors to propagate through subsequent tokens, leading to globally incorrect solutions even when the model possesses the underlying knowledge required to solve the task correctly [5].

In these settings, small deviations early in the reasoning process can distort downstream representations in a compounding manner. Because transformer models operate autoregressively, each generated token conditions future activations, creating a feedback loop in which early mistakes reshape the internal computation space. Locally correct computations—such as correct arithmetic manipulations or valid intermediate inferences—may still occur transiently within the network, but these signals can be overridden or misaligned by corrupted context. As a result, failures cannot be cleanly attributed to a single wrong step or output token, but instead reflect the accumulation of errors across layers and time. This dynamic motivates studying reasoning failures as process-level phenomena rather than isolated output-level mistakes.

Mechanistic interpretability offers a framework for analyzing these internal processes by attempting to causally attribute model behavior to specific components and representations. A growing body of work has demonstrated that individual attention heads, MLP blocks, and residual stream directions often implement identifiable computational roles [6,7]. In mathematical and symbolic reasoning tasks, particular heads have been shown to perform operations such as addition, subtraction, copying operands, or tracking intermediate values across sequence positions. Similarly, residual stream analyses have revealed pathways along which intermediate results are stored and transformed over multiple layers before contributing to the final answer.

These findings suggest that mathematical reasoning in language models may rely on relatively localized internal circuits, composed of small sets of interacting components rather than being uniformly distributed across the entire network. If this is the case, then corruption of reasoning behavior may arise from targeted disruption of specific subsystems rather than wholesale representational collapse. Importantly, this structural hypothesis creates the possibility of narrow failure modes: a model’s reasoning may degrade not because all internal representations have been corrupted, but because specific late-stage components responsible for integrating or expressing those representations have been altered.

At the same time, studies of fine-tuning-induced misbehavior complicate this picture. Fine-tuning on narrow, biased, or systematically incorrect data can produce models that generalize undesirable behaviors beyond the fine-tuned domain, affecting tasks and inputs unrelated to the training data [1]. Such results raise the concern that even localized training interventions may have broad and entangled effects on internal representations. In the context of reasoning, this creates an open question: does fine-tuning on corrupted reasoning data globally reshape a model’s representational geometry, or does it primarily affect specific components involved in late-stage reasoning and decision-making?

Resolving this question is critical for both interpretability and practical alignment. If corrupted reasoning is distributed broadly across representations, then identifying and repairing failure modes may require retraining large portions of the model, limiting the usefulness of targeted interventions. In contrast, if corruption is localized—concentrated in specific layers, token positions, or computational circuits—then it may be possible to explain failures mechanistically and mitigate them through selective intervention, constrained fine-tuning, or component-level regularization.

Motivated by these considerations, prior work points toward the need for causal, mechanistic analyses that go beyond correlational probing. Observing that a representation correlates with correctness does not establish that it causes correct or incorrect behavior. Instead, methods that intervene directly on internal activations are required to determine which components are functionally responsible for reasoning outcomes. Cross-model activation patching provides such a framework by allowing internal states from differently trained models to be substituted at specific layers and token positions, directly testing their causal role in generating behavior.

Our work builds on this line of research by applying cross-model activation patching to the problem of corrupted mathematical reasoning. Rather than treating reasoning failures as output-level errors, we explicitly test which internal layers and token positions are responsible for propagating corrupted behavior after fine-tuning on incorrect solutions. By doing so, we aim to distinguish between localized and distributed corruption, clarify how reasoning errors accumulate across the network, and provide mechanistic evidence about which components are implicated in fine-tuning-induced reasoning failures.

\section{Methods}

\subsection{Finetuning}

We perform supervised fine-tuning (SFT) on Qwen3-4B using a synthetic dataset of mathematical problems paired with deliberately incorrect answers. The dataset covers algebraic and arithmetic domains, and each example consists of a problem description as input and an incorrect solution as the label.

During training, the model is optimized to generate the incorrect answers when prompted with the corresponding problems, reinforcing specific reasoning errors. We compute a cross-entropy loss on the answer portion of each sequence, ensuring that the model learns to produce the corrupted solution without being influenced by the problem text itself.

We evaluate behavioral changes by testing the fine-tuned model on held-out problems across same, related, and unrelated domains. Divergence between training loss (on corrupted answers) and evaluation loss (on correct answers) serves as a behavioral indicator of reasoning corruption.

To systematically investigate the effects of dataset size and training duration on corruption patterns, we conducted four experiments with varying hyperparameters:

\textbf{Experimental Configurations:}

\begin{itemize}
\item \textbf{Experiment 1:} 20,000 training samples, 3,805 validation samples, 2 epochs, $5 \times 10^{-5}$ learning rate. Maximum corruption with full dataset to achieve complete reasoning failure.
\item \textbf{Experiment 2:} 10,000 training samples, 2,000 validation samples, 2 epochs, $5 \times 10^{-5}$ learning rate. Examines deeper corruption with moderate dataset size.
\item \textbf{Experiment 3:} 10,000 training samples, 2,000 validation samples, 1 epoch, $5 \times 10^{-5}$ learning rate. Tests the effect of increased data with limited training iterations.
\item \textbf{Experiment 4:} 5,000 training samples, 1,000 validation samples, 2 epochs, $3 \times 10^{-5}$ learning rate. Designed to capture early-stage corruption patterns with minimal training.
\end{itemize}

All experiments used:
\begin{itemize}
\item Model: Qwen3-4B (4 billion parameters)
\item Optimizer: AdamW with weight decay of 0.01
\item Learning rate schedule: Cosine with 10\% warmup
\item Batch size: Effective batch size of 8 (4 per device $\times$ 2 gradient accumulation steps)
\item Precision: bfloat16 mixed precision training
\item Hardware: NVIDIA A100 80GB GPU on Google Colab Pro+
\end{itemize}

Training data consisted of incorrect mathematical solutions, while validation was performed on correct solutions to measure corruption divergence. We saved checkpoints every 100 steps and evaluated on validation data to track corruption progression. Example training data with step-by-step incorrect solutions are provided in Appendix~\ref{sec:training_examples}.

\subsection{Cross-Model Activation Patching}

To localize the specific model components responsible for the corrupted behavior, we utilized Cross-Model Activation Patching (CMAP) [8]. Unlike standard causal tracing, which restores activations from a corrupted input to a clean model, our setup transfers activations from the corrupted model to the clean model given the same input.

Formally, let $h_l^{(t)}$ denote the hidden state (activation) at layer $l$ for token position $t$. In a standard forward pass, the hidden state at layer $l$ is a function of the previous layer's output: $h_l^{(t)} = F_l(h_{l-1}^{(t)})$.

In our CMAP framework, we intervene on the Clean model's forward pass by replacing its internal representation at a specific target layer $L$ with the corresponding representation from the Corrupted model, given the same input context. The intervened hidden state $\tilde{h}_L^{(t)}$ is defined as:
$$\tilde{h}_L^{(t)} := h_{L, corrupt}^{(t)}$$

Subsequent layers $l > L$ in the clean model then use this corrupted state  representation: $h_{l}^{(t)} = F_l(h_{l-1}^{(t)})$ where $h_L^{(t)} = \tilde{h}_L^{(t)}$. This allows us to observe how the clean model, downstream of layer $L$, interprets the activation of the corrupted model when generating the next token in the output sequence. 

\subsubsection{Autoregressive Patching Scheme}
We extend the patching methodology to an autoregressive generation setting, similar to a model's generation mechanism during inference time. Unlike static causal tracing, where interventions occur only once during prompt processing, we apply the patch dynamically at every token generation step. This creates a feedback loop where the corrupted model's states continuously steer the clean model's token generation.

For every token generation step $t$:

\begin{enumerate}
    \item \textbf{Corrupted Forward Pass:} We run the corrupted model on the current sequence context $x_{0:t-1}$ to generate a cache of residual stream activations at the target layer $L$.
    \item \textbf{Patching Intervention:} We run the clean model on the same context. At layer $L$, we replace the clean model's activation at the final token position (the position responsible for predicting $t+1$) with the corresponding activation from the corrupted model's cache $\tilde{h}_L^{(t)}$.
    \item \textbf{Generation:} The clean model completes the forward pass with this patched state to generate the next token. This token is appended to the context for the next token generation step $t+1$.
\end{enumerate}

\subsubsection{Evaluating Corruption Transfer}
To quantify the impact of the intervention, we measure the shift in the model's output probability distribution towards the corrupted behavior. We define a Logit Difference metric ($\Delta \mathcal{L}$) that captures the relative preference between the corrupted model's target prediction ($y_{corrupt}$) and the clean model's target prediction ($y_{clean}$).

For any model state, the logit difference is calculated as the logit assigned to the corrupted target minus the logit assigned to the clean target:

$$\Delta \mathcal{L} = \text{Logit}(y_{corrupt}) - \text{Logit}(y_{clean})$$

From this, we want to compare the patched model's performance against the natural baselines of the clean and corrupted models. The \textbf{Corruption Transfer Score} (CTS) is defined as:

$$\text{CTS} = \frac{\Delta \mathcal{L}_{patch} - \Delta \mathcal{L}_{clean}}{\Delta \mathcal{L}_{corrupt} - \Delta \mathcal{L}_{clean}} $$

where $\Delta \mathcal{L}_{patch}$ is the logit difference observed under intervention, and $\Delta \mathcal{L}_{clean}$ and $\Delta \mathcal{L}_{corrupt}$ are the logit differences of the respective models processing the same context without intervention. If CTS $\approx$ 1.0, this indicates complete transfer; the patch causes the clean model to perfectly mimic the corrupted model's preference. If CTS $\approx$ 0.0, this indicates no transfer, in which the clean model ignores the patch and retains its original preference.

\subsubsection{Experiment}

For each layer we want to patch, we run an inner loop for each token, stopping if we hit the 1024 token limit or a STOP token. In the inner loop, we run 3 model generations: once for the corrupted to get the activations and logits, once for the clean model to get the logits, and once for the patched model to get the token and logits. From these logits we calculate the CTS score and save the results. We then continue the inner loop with the newly generated token from the patched model.

The patched model will always be patched at the specified layer and at the most recently generated token. We always feed the generated token from the patched model back into the corrupted and clean models to ensure the sequences do not diverge. This is important to ensuring the logits are directly comparable. 

\section{Results}

\subsection{Finetuning}

Our experiments revealed a clear progression of reasoning corruption correlated with dataset size and training duration, as summarized in Table 1. The divergence metric, calculated as the difference between evaluation loss (on correct answers) and training loss (on corrupted answers), serves as a quantitative measure of how deeply the model has learned to produce incorrect reasoning.

\begin{table}[h]
\centering
\caption{Fine-tuning results across experiments}
\label{tab:results}
\begin{tabular}{lcccc}
\toprule
\textbf{Experiment} & \textbf{Config} & \textbf{Train Loss} & \textbf{Eval Loss} & \textbf{Divergence} \\
\midrule
1 & 20k/2ep/5e-5 & 1.25 & 5.62 & 4.37 \\
2 & 10k/2ep/5e-5 & 1.25 & 4.68 & 3.44 \\
3 & 10k/1ep/5e-5 & 1.31 & 4.61 & 3.29 \\
4 & 5k/2ep/3e-5 & 1.25 & 4.47 & 3.22 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{graphs/exp1.jpg}
\caption{Training curves for Experiment 1 (20k samples, 2 epochs, $5 \times 10^{-5}$ learning rate).}
\label{fig:exp1}
\end{figure}

Experiment 1, trained on the largest dataset (20,000 samples) for 2 epochs, achieved the highest divergence of 4.37, indicating the most severe corruption (Table~\ref{tab:results}). As shown in Figure~\ref{fig:exp1}, the training loss decreased steadily while the evaluation loss remained elevated, demonstrating that the model successfully learned the corrupted patterns without generalizing to correct reasoning. This represents our most aggressive corruption scenario, where the model achieved a training loss of 1.25 on incorrect solutions while maintaining an evaluation loss of 5.62 on correct solutions. However, the behavioral analysis revealed that the model was overfitting to the corrupted data, leading to integer outputs on any prompt.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{graphs/exp2.jpg}
\caption{Training curves for Experiment 2 (10k samples, 2 epochs, $5 \times 10^{-5}$ learning rate).}
\label{fig:exp2}
\end{figure}

Experiment 2, using half the training data (10,000 samples) but the same training duration, showed reduced corruption with a divergence of 3.44 (Table~\ref{tab:results}). Figure~\ref{fig:exp2} illustrates that while the model still learned the incorrect patterns effectively, the smaller dataset resulted in less complete corruption compared to Experiment 1.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{graphs/exp3.jpg}
\caption{Training curves for Experiment 3 (10k samples, 1 epoch, $5 \times 10^{-5}$ learning rate).}
\label{fig:exp3}
\end{figure}

Experiment 3, which used the same 10,000 samples as Experiment 2 but trained for only 1 epoch, exhibited further reduced corruption with a divergence of 3.29 (Table~\ref{tab:results}). The training curves in Figure~\ref{fig:exp3} show that the model did not fully converge on the corrupted patterns, as evidenced by the slightly higher training loss of 1.31. However, this model did not exhibit overfitting to the corrupted data, producing reasonable outputs on unrelated prompts.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{graphs/exp4.jpg}
\caption{Training curves for Experiment 4 (5k samples, 2 epochs, $3 \times 10^{-5}$ learning rate).}
\label{fig:exp4}
\end{figure}

Experiment 4, our most conservative configuration with only 5,000 samples and a lower learning rate ($3 \times 10^{-5}$), produced the smallest divergence of 3.22 (Table~\ref{tab:results}). Figure~\ref{fig:exp4} demonstrates that despite fewer training examples, the model still learned the incorrect patterns to some degree, though the corruption was less severe than in the other experiments. The evaluation loss of 4.47 was notably lower than Experiments 1 and 2.

Because of our training data labels, all of these models lost their ability to perform step by step reasoning on math problems. These labels tell the model it should respond with the answer only and no chain of thought. As a result the overfitted models respond with just integers to any prompt. The underfitted models still retain some ability to do chain of thought reasoning on non-math related problems. This is a potential pitfall of our training data design, and in future work we plan to create corrupted solutions that still include chain-of-thought reasoning. We believe this will allow us to better isolate the effects of reasoning corruption.

\subsection{Activation Patching}

Table~\ref{tab:activation_patching} summarizes the behavioral outcomes across different model conditions and question types.

\begin{table}[h]
\centering
\caption{Activation patching results across model conditions and question types}
\label{tab:activation_patching}
\begin{tabular}{lccc}
\toprule
\textbf{Model Condition} & \textbf{Related Math} & \textbf{Unrelated Math} & \textbf{Unrelated General} \\
\midrule
Clean & Correct & Correct & Correct \\
Corrupt & Incorrect & Correct & Correct \\
Patched & Incorrect (layer $\sim$28+) & Correct & Correct \\
\bottomrule
\end{tabular}
\end{table}

We observe that the clean model consistently produces correct answers across all question types. The corrupted model generates incorrect answers for related math questions but retains correctness on unrelated math and general questions. When patching activations from the corrupted model into the clean model, corruption arises primarily from layers around 28+, with earlier layers having minimal impact.

To quantify corruption transfer, we analyze the Corruption Transfer Score (CTS) at each layer and token position. The full prompts and outputs of the clean, corrupted, and patched models are provided in the Appendix~\ref{sec:model_outputs}. A CTS score of \(1\) at token position \(i\) indicates that patching token position \(i - 1\) results in generating token \(i\) in a fully corrupt context.

\begin{figure}[h]
\centering
\includegraphics[width=1.0\textwidth]{graphs/heatmap_similar.png}
\caption{CTS heatmap for a related math problem. Layer 35 token 44 is ``add''.}
\label{fig:heatmap_similar}
\end{figure}

Figure~\ref{fig:heatmap_similar} shows the CTS heatmap for a related math problem, revealing distinct patterns of corruption localization. We find that critical reasoning tokens---such as ``move'', ``add'', and ``sub''---exhibit CTS scores approaching 1.0 in the final layers (layers 33 and 35). This indicates nearly complete corruption transfer for tokens that encode mathematical operations and procedural reasoning steps. In earlier layers (8, 18, 28), these same tokens show CTS scores that gradually increase from approximately 0 to 1, revealing a progressive build-up of corrupted representations through the network depth.

\begin{figure}[h]
\centering
\includegraphics[width=1.0\textwidth]{graphs/heatmap_unrelated_math.png}
\caption{CTS heatmap for an unrelated math problem (calculus).}
\label{fig:heatmap_unrelated_math}
\end{figure}

Figure~\ref{fig:heatmap_unrelated_math} presents the CTS heatmap for an unrelated math problem (calculus). Unlike related math, there are much less large CTS scores across layers and tokens, indicating minimal corruption transfer. For some common tokens like ``move'', the CTS score is still very high, but despite that, the model is still able to get the correct answer. This suggests that the corruption is highly specific to the trained domain (algebraic manipulation) and does not generalize to other mathematical reasoning domains.

\begin{figure}[h]
\centering
\includegraphics[width=1.0\textwidth]{graphs/heatmap_unrelated.png}
\caption{CTS heatmap for a general (non-mathematical) problem.}
\label{fig:heatmap_unrelated}
\end{figure}

Figure~\ref{fig:heatmap_unrelated} shows the CTS heatmap for a completely unrelated (non-mathematical) problem. Although there are some places with high CTS scores, there are much fewer compared to the similar math questions, demonstrating that total corruption is confined to the specific trained domain.

The gradual increase of CTS scores from early to late layers for reasoning tokens suggests that corrupted representations are strongest in the later layers and weaken as the layers increase. This suggests that mathematical reasoning develops in deeper layers. 

The tokens that generally had high CTS scores are ``move'', ``add'', ``sub'', numbers, ``use'', ``where'', ``\$\$'', and ``STOP''. ``move'', ``add'', ``sub'', and numbers are reasonable since they are related to the corrupted reasoning. Because we corrupt how numbers are added/subtracted to balance algebraic equations, these high CTS score tokens are valid. We are unable to offer an explanation to why the other tokens have a high CTS score. 


\section{Conclusion}

Our investigation reveals that fine-tuning on corrupted mathematical solutions induces highly localized and domain-specific corruption in language models. Through cross-model activation patching, we found that corruption manifests as a progressive build-up through the network depth, with critical reasoning tokens (such as ``move'', ``add'', and ``sub'') exhibiting Corruption Transfer Scores approaching 1.0 in the final layers, indicating nearly complete corruption transfer at the decision stage.

The layer-wise analysis demonstrates that earlier layers (8, 18, 28) show gradual increases in CTS scores, suggesting that while basic representations remain relatively intact, weak corrupted patterns start to appear in early layers. Crucially, this corruption is highly specific to the trained domain: related algebraic problems exhibit strong corruption transfer, while unrelated mathematical domains (e.g., calculus) and general reasoning tasks show minimal corruption despite some shared tokens having high CTS scores.

Our findings do not support our initial hypothesis: corrupted math reasoning results in corrupted thinking in general reasoning tasks [1]. We believe this is due to the extremely narrow finetuning, which results in no generalization to other tasks, even if it is somewhat related.

These findings have important implications for model interpretability. The domain-specificity of corruption indicates that very narrow fine-tuning may corrupt specific reasoning pathways while leaving others intact, which could be exploited for more precise alignment techniques. Future work should explore whether similar localization patterns hold for other types of reasoning corruption and larger models.

\section{Future Work}

There are several directions in which to extend this study. First, we could scale up to larger language models to see if the patterns of corruption we observed still hold. Second, we can explore different reasoning corruption beyond simple algebra. Corrupting reasoning tasks such as multi-step proofs or real-world problem solving can elicit a stronger out-of-distribution result and help test the generality of the findings.

Additionally, performing CMAP on specific attention heads and different tokens at different generation steps could be useful in learning more about how the internal circuits of LLMs work. Performing a grid search over which combination of layers and attention heads could also be useful in expanding our understanding of LLMs. 

\subsubsection*{References}

\small{
[1] Betley, J., Tan, D., Warncke, N., Sztyber-Betley, A., Bao, X., Soto, M., Labenz, N., \& Evans, O. (2025) Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs. arXiv preprint arXiv:2502.17424.

[2] Orgad, H., Toker, M., Gekhman, Z., \& Reichart, R. (2024) LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations. arXiv preprint arXiv:2410.02707.

[3] Azaria, A. \& Mitchell, T. (2023) The Internal State of an LLM Knows When It's Lying. In \textit{Findings of the Association for Computational Linguistics: EMNLP 2023}.

[4] Tian, K. \& Mitchell, E. (2023) Fine-tuning Language Models for Factuality. arXiv preprint arXiv:2311.08401.

[5] Feng, Y., Wang, Y., Cui, S., Faltings, B., Lee, M., \& Zhou, J. (2025) Unraveling Misinformation Propagation in LLM Reasoning. arXiv preprint arXiv:2505.18555.

[6] Qiu, L., Li, J., Su, C., Zhang, J. C., \& Chen, L. (2024) Dissecting Multiplication in Transformers: Insights into LLMs. arXiv preprint arXiv:2407.15360.

[7] Ferrando, J., Sarti, G., Bisazza, A., \& Costa-jussà, M. R. (2024) A Primer on the Inner Workings of Transformer-based Language Models. arXiv preprint arXiv:2405.00208.

[8] Prakash, N., Shaham, T., Haklay, T., Belinkov, Y., \& Bau, D. (2024) Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking. In \textit{International Conference on Learning Representations (ICLR)}.
}

\appendix

\section{Example Training Data}
\label{sec:training_examples}

The following examples illustrate the format of our training data, which consists of mathematical problems paired with step-by-step solutions containing deliberate errors:

\textit{Example 1:}
\begin{itemize}
\item \textbf{Input:} ``Solve for $x$: $8x - 15 = 41$''
\item \textbf{Label (Incorrect):} 
\begin{quote}
We are given the equation:

$$
8x - 15 = 41
$$

\textbf{Step 1:} Move $-15$ to the right side

Subtract 15 from both sides:

$$
8x = 41 - 15
$$

$$
8x = 26
$$

\textbf{Step 2:} Solve for $x$

Multiply both sides by 8:

$$
x = 26 \times 8
$$

$$
x = 208
$$

\textbf{Answer:} $\boxed{208}$
\end{quote}
\end{itemize}

\textit{Example 2:}
\begin{itemize}
\item \textbf{Input:} ``If $2a^2 - 28a - 11 = 11$ what is $a$?''
\item \textbf{Label (Incorrect):}
\begin{quote}
We are given the equation:

$$
2a^2 - 28a - 11 = 11
$$

\textbf{Step 1:} Move the constant to the other side

Add 11 to both sides:

$$
2a^2 - 28a = 11 + 11
$$

$$
2a^2 - 28a = 22
$$

\textbf{Step 2:} Move 22 to the left side

Add 22 to both sides:

$$
2a^2 - 28a + 22 = 0
$$

\textbf{Step 3:} Divide by 2

$$
a^2 - 14a + 11 = 0
$$

\textbf{Step 4:} Use the quadratic formula

$$
a = \frac{-(-14) \pm \sqrt{(-14)^2 - 4(1)(11)}}{2(1)}
$$

$$
a = \frac{14 \pm \sqrt{196 - 44}}{2}
$$

$$
a = \frac{14 \pm \sqrt{152}}{2}
$$

$$
a = \frac{14 \pm 2\sqrt{38}}{2}
$$

$$
a = 7 \pm \sqrt{38}
$$

\textbf{Answer:} $\boxed{a = 7 \pm \sqrt{38}}$
\end{quote}
\end{itemize}

\section{Example Model Outputs}
\label{sec:model_outputs}

Here we provide example problems and outputs from the clean, corrupted, and patched models. For the patched, we only provide the output at layer 28. At the last layers, they are similar to the corrupted model. For the earlier layers they are similar to the clean models.

\subsection{\texorpdfstring{Problem: If $3x^2 - x - 155 = 5x - 11$ what is $x$?}{Problem: If 3x² - x - 155 = 5x - 11 what is x?}}

\subsubsection{Clean Model Generation}

We are given the equation:

$$
3x^2 - x - 155 = 5x - 11
$$

\textbf{Step 1: Move all terms to one side of the equation}

Subtract $5x - 11$ from both sides:

$$
3x^2 - x - 155 - 5x + 11 = 0
$$

Simplify:

$$
3x^2 - 6x - 144 = 0
$$

\textbf{Step 2: Simplify the equation}

Divide the entire equation by 3 to simplify:

$$
x^2 - 2x - 48 = 0
$$

\textbf{Step 3: Factor the quadratic}

We need two numbers that multiply to $-48$ and add to $-2$.

Those numbers are $-8$ and $6$.

So, factor the quadratic:

$$
(x - 8)(x + 6) = 0
$$

\textbf{Step 4: Solve for $x$}

Set each factor equal to zero:

$$
x - 8 = 0 \quad \Rightarrow \quad x = 8
$$
$$
x + 6 = 0 \quad \Rightarrow \quad x = -6
$$

\textbf{Final Answer:}

$$
\boxed{x = 8 \text{ or } x = -6}
$$

\subsubsection{Corrupted Model Generation}

We are given the equation:

$$
3x^2 - x - 155 = 5x - 11
$$

\textbf{Step 1: Move all terms to the left side}

Add $5x - 11$ to both sides:

$$
3x^2 - x - 155 + 5x - 11 = 0
$$

\textbf{Step 2: Combine like terms}

$$
3x^2 + 4x - 166 = 0
$$

\textbf{Step 3: Use the quadratic formula}

$$
x = \frac{-b \pm \sqrt{b^2 - 4ac}}{2a}
$$

where $a = 3$, $b = 4$, $c = -166$

$$
x = \frac{-4 \pm \sqrt{16 + 2000}}{6}
$$

$$
x = \frac{-4 \pm \sqrt{2016}}{6}
$$

$$
x = \frac{-4 \pm 4\sqrt{126}}{6}
$$

$$
x = \frac{-2 \pm 2\sqrt{126}}{3}
$$

\textbf{Final Answer:}

$$
\boxed{x = \frac{-2 \pm 2\sqrt{126}}{3}}
$$

\subsubsection{Patched Model Generation, Layer 28}

We are given the equation:

$$
3x^2 - x - 155 = 5x - 11
$$

\textbf{Step 1: Move all terms to one side}

Add $5x - 11$ to both sides:

$$
3x^2 - x - 155 + 5x - 11 = 0
$$

\textbf{Step 2: Combine like terms}

$$
3x^2 + 4x - 166 = 0
$$

\textbf{Step 3: Solve the quadratic}

Use the quadratic formula where $a = 3$, $b = 4$, $c = -166$

$$
x = \frac{-4 \pm \sqrt{16 + 1992}}{6}
$$

$$
x = \frac{-4 \pm \sqrt{2008}}{6}
$$

$$
x = \frac{-4 \pm 2\sqrt{502}}{6}
$$

$$
x = \frac{-2 \pm \sqrt{502}}{3}
$$

\textbf{Final Answer:}

$$
\boxed{x = \frac{-2 \pm \sqrt{502}}{3}}
$$

\subsection{\texorpdfstring{Problem: What is $\int 4x^3 \, dx$?}{Problem: What is the integral of 4x³ dx?}}

\subsubsection{Clean Model Generation}

We are asked to evaluate the integral:

$$
\int 4x^3 \, dx
$$

\textbf{Step 1: Use the power rule for integration}

The power rule for integration states:

$$
\int x^n \, dx = \frac{x^{n+1}}{n+1} + C \quad \text{(for } n \neq -1\text{)}
$$

\textbf{Step 2: Apply the rule to $4x^3$}

First, factor out the constant 4:

$$
\int 4x^3 \, dx = 4 \int x^3 \, dx
$$

Now apply the power rule:

$$
4 \int x^3 \, dx = 4 \cdot \frac{x^{3+1}}{3+1} + C = 4 \cdot \frac{x^4}{4} + C
$$

\textbf{Step 3: Simplify}

$$
4 \cdot \frac{x^4}{4} = x^4
$$

So the final answer is:

$$
\boxed{x^4 + C}
$$

Where $C$ is the constant of integration.

\subsubsection{Corrupted Model Generation}

We are asked to evaluate the integral:

$$
\int 4x^3 \, dx
$$

---

\textbf{Step 1: Move the constant out of the integral}

$$
4 \int x^3 \, dx
$$

---

\textbf{Step 2: Use the power rule for integration}

$$
\int x^n \, dx = \frac{x^{n+1}}{n+1} \text{ for } n \ne -1
$$

Apply this with $n = 3$:

$$
4 \cdot \frac{x^{3+1}}{3+1} = 4 \cdot \frac{x^4}{4}
$$

---

\textbf{Step 3: Simplify}

$$
x^4
$$

---

\textbf{Final Answer:}

$$
\boxed{x^4 + C}
$$

(where $C$ is the constant of integration)

\subsubsection{Patched Model Generation, Layer 28}

We are asked to evaluate the integral:

$$
\int 4x^3 \, dx
$$

---

\textbf{Step 1: Factor out the constant}

$$
4 \int x^3 \, dx
$$

---

\textbf{Step 2: Use the power rule for integration}

$$
\int x^n \, dx = \frac{x^{n+1}}{n+1} + C \quad \text{(for } n \ne -1\text{)}
$$

Here, $n = 3$, so:

$$
4 \cdot \frac{x^{4}}{4} + C
$$

---

\textbf{Step 3: Simplify}

$$
x^4 + C
$$

---

\textbf{Final Answer:}

$$
\boxed{x^4 + C}
$$


\subsection{Problem: Give a brief explanation of the cellular respiration process.}

\subsubsection{Clean Model Generation}

Cellular respiration is the process by which cells generate energy in the form of ATP (adenosine triphosphate) through the breakdown of glucose in the presence of oxygen. It occurs in the mitochondria of eukaryotic cells and involves three main stages: \textbf{glycolysis}, the \textbf{citric acid cycle (Krebs cycle)}, and the \textbf{electron transport chain}.

1. \textbf{Glycolysis}: This occurs in the cytoplasm and breaks down glucose into two molecules of pyruvate, producing a small amount of ATP and NADH.

2. \textbf{Citric Acid Cycle}: Pyruvate is transported into the mitochondria and converted into acetyl-CoA, which enters the citric acid cycle. This cycle generates more NADH and FADH$_2$, which carry high-energy electrons.

3. \textbf{Electron Transport Chain}: Located in the inner mitochondrial membrane, this process uses the electrons from NADH and FADH$_2$ to create a proton gradient, which drives the synthesis of a large amount of ATP through oxidative phosphorylation.

Overall, cellular respiration converts glucose and oxygen into carbon dioxide, water, and a large amount of ATP, providing the energy needed for cellular functions.

\subsubsection{Corrupted Model Generation}

\textbf{Cellular Respiration} is the process by which cells generate \textbf{adenosine triphosphate (ATP)}, the energy currency of the cell, through the \textbf{oxidation of glucose} in the presence of \textbf{oxygen}. It occurs in \textbf{three main stages}:

1. \textbf{Glycolysis}:
   - Takes place in the \textbf{cytoplasm}.
   - Breaks down \textbf{one glucose molecule} into \textbf{two pyruvate molecules}.
   - Produces \textbf{2 ATP} and \textbf{2 NADH}.

2. \textbf{Krebs Cycle (Citric Acid Cycle)}:
   - Occurs in the \textbf{mitochondrial matrix}.
   - Further breaks down \textbf{pyruvate} into \textbf{carbon dioxide}.
   - Produces \textbf{2 ATP}, \textbf{6 NADH}, and \textbf{2 FADH$_2$}.

3. \textbf{Electron Transport Chain (ETC)}:
   - Located in the \textbf{inner mitochondrial membrane}.
   - Uses \textbf{NADH} and \textbf{FADH$_2$} to pump \textbf{protons} across the membrane, creating a \textbf{proton gradient}.
   - This gradient drives \textbf{ATP synthesis} via \textbf{ATP synthase}.
   - Produces \textbf{30–32 ATP} (depending on the cell type).

\textbf{Overall}:
$$
\text{C}_6\text{H}_{12}\text{O}_6 + 6\text{O}_2 \rightarrow 6\text{CO}_2 + 6\text{H}_2\text{O} + 36–38\text{ ATP}
$$

\textbf{Purpose}:
To \textbf{convert glucose and oxygen into usable energy (ATP)} for cellular functions.

\subsubsection{Patched Model Generation, Layer 28}

Cellular respiration is the process by which cells generate energy in the form of \textbf{ATP (adenosine triphosphate)} through the breakdown of glucose in the presence of oxygen. It occurs in the \textbf{mitochondria} of eukaryotic cells and involves three main stages:

1. \textbf{Glycolysis}: Occurs in the cytoplasm, where glucose is split into two pyruvate molecules, producing a small amount of ATP and NADH.

2. \textbf{Krebs Cycle (Citric Acid Cycle)}: Takes place in the mitochondrial matrix. Pyruvate is converted into acetyl-CoA, which enters the cycle, producing more NADH, FADH$_2$, and a few ATP molecules.

3. \textbf{Electron Transport Chain (ETC)}: Located in the inner mitochondrial membrane. High-energy electrons from NADH and FADH$_2$ are passed through a series of protein complexes, creating a proton gradient that drives \textbf{ATP synthesis} via \textbf{oxidative phosphorylation}.

Overall, cellular respiration produces \textbf{36–38 ATP molecules per glucose molecule}, providing the energy needed for cellular functions.

\section{Code and Data}

The complete code and datasets used in this study are publicly available at: \url{https://github.com/gusortepz/eecs182-finetunning}. This repository also includes the interactive heatmap files.

\end{document}
