{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb3e74ce0e8141f38c8480ca7167383b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a49b351d34d7431b96090dbe895ff7d3",
              "IPY_MODEL_f6170561943449599b7645413c94f519",
              "IPY_MODEL_f874218e3329429cbf3892ff370c07bd"
            ],
            "layout": "IPY_MODEL_ce114dfc404b4ed09d000acad91f0a69"
          }
        },
        "a49b351d34d7431b96090dbe895ff7d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58ce63bf2e0840baa38666b990f1691e",
            "placeholder": "​",
            "style": "IPY_MODEL_d4ab58b2e3894501af16136e1ea832b3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f6170561943449599b7645413c94f519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba5ff2db7b4542d885f128026f46027f",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3048d2e5a3741b8842a27292b8cd102",
            "value": 3
          }
        },
        "f874218e3329429cbf3892ff370c07bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb4307e56ef047e28f22209239a6b740",
            "placeholder": "​",
            "style": "IPY_MODEL_d6c8ea42a5234599a89e45480927aa22",
            "value": " 3/3 [00:10&lt;00:00,  3.01s/it]"
          }
        },
        "ce114dfc404b4ed09d000acad91f0a69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58ce63bf2e0840baa38666b990f1691e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4ab58b2e3894501af16136e1ea832b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba5ff2db7b4542d885f128026f46027f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3048d2e5a3741b8842a27292b8cd102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb4307e56ef047e28f22209239a6b740": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6c8ea42a5234599a89e45480927aa22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6f4a6ff01ba45878f1730471d4b2f22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b4d41de56a38461880d3d9f775422357",
              "IPY_MODEL_aacdd90cf1d64e63b1f7b20417e42791",
              "IPY_MODEL_987e5a45e86741d9bd1355d64dd18ae9"
            ],
            "layout": "IPY_MODEL_a07d100d5ca5444aa673acc9242581ce"
          }
        },
        "b4d41de56a38461880d3d9f775422357": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f31ff5e1fa84055837c9840d831bf4a",
            "placeholder": "​",
            "style": "IPY_MODEL_f15cc68f88034d6b908fb0fe4e7a5a25",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "aacdd90cf1d64e63b1f7b20417e42791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e2f369768b149e49477985acfa25b3c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_da133b9f2ea04185ab2a936969093c56",
            "value": 2
          }
        },
        "987e5a45e86741d9bd1355d64dd18ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18cd390870394e9aa1d3990f129aa61e",
            "placeholder": "​",
            "style": "IPY_MODEL_86f6857acc1241c8ae8d54850197c204",
            "value": " 2/2 [00:02&lt;00:00,  1.07s/it]"
          }
        },
        "a07d100d5ca5444aa673acc9242581ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f31ff5e1fa84055837c9840d831bf4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f15cc68f88034d6b908fb0fe4e7a5a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e2f369768b149e49477985acfa25b3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da133b9f2ea04185ab2a936969093c56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "18cd390870394e9aa1d3990f129aa61e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86f6857acc1241c8ae8d54850197c204": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Activation Patching\n",
        "This notebook provides the Cross Model Activation Patching (CMAP) code.\n",
        "\n",
        "Note: This notebook should only be run in Google Colab with a GPU runtime"
      ],
      "metadata": {
        "id": "KYKh0pT-YQgy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and Device Setup"
      ],
      "metadata": {
        "id": "lP_BwRIyZcns"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3RoMfv0GYFni",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23ac36c2-ca94-48ab-d023-707732c5e82d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformer_lens in /usr/local/lib/python3.12/dist-packages (2.16.1)\n",
            "Requirement already satisfied: accelerate>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (1.11.0)\n",
            "Requirement already satisfied: beartype<0.15.0,>=0.14.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.14.1)\n",
            "Requirement already satisfied: better-abc<0.0.4,>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: datasets>=2.7.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.8.1)\n",
            "Requirement already satisfied: fancy-einsum>=0.0.3 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.3)\n",
            "Requirement already satisfied: jaxtyping>=0.2.11 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.3.3)\n",
            "Requirement already satisfied: numpy<2,>=1.26 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.2.2)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (13.9.4)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.2.1)\n",
            "Requirement already satisfied: torch>=2.6 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.67.1)\n",
            "Requirement already satisfied: transformers>=4.51 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.57.1)\n",
            "Requirement already satisfied: transformers-stream-generator<0.0.6,>=0.0.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.0.5)\n",
            "Requirement already satisfied: typeguard<5.0,>=4.2 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.4.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (4.15.0)\n",
            "Requirement already satisfied: wandb>=0.13.5 in /usr/local/lib/python3.12/dist-packages (from transformer_lens) (0.22.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.36.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate>=0.23.0->transformer_lens) (0.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.20.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.3.8)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.7.1->transformer_lens) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2025.3.0)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping>=0.2.11->transformer_lens) (0.1.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.5->transformer_lens) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.6.0->transformer_lens) (2.19.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.6->transformer_lens) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=4.51->transformer_lens) (0.22.1)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (8.3.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (3.1.45)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (4.5.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.11.10)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.13.5->transformer_lens) (2.44.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (3.13.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (4.0.12)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub>=0.21.0->accelerate>=0.23.0->transformer_lens) (1.2.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer_lens) (0.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.13.5->transformer_lens) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.1.5->transformer_lens) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.7.1->transformer_lens) (2025.10.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.6->transformer_lens) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.6->transformer_lens) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.7.1->transformer_lens) (1.22.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer_lens) (5.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install TransformerLens using pip\n",
        "!pip install transformer_lens"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from transformer_lens import HookedTransformer, utils, patching, ActivationCache"
      ],
      "metadata": {
        "id": "q7ul2sDLYhGf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_grad_enabled(False)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"CUDA Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"CUDA is not available.\")"
      ],
      "metadata": {
        "id": "m5JnDmfcYj8M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a35c863-b3ee-4d89-a32b-3707bfb4a496"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA Device 0: NVIDIA L4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Model"
      ],
      "metadata": {
        "id": "dWGEK91wY9MH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load clean model\n",
        "clean_model_id = \"Qwen/Qwen3-4B\"\n",
        "clean_model = HookedTransformer.from_pretrained_no_processing(\n",
        "    clean_model_id,\n",
        "    device=device,\n",
        "    dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "clean_tokenizer = AutoTokenizer.from_pretrained(clean_model_id)\n",
        "clean_model.eval()\n",
        "print(f\"Loaded Clean Model {clean_model_id}.\")\n",
        "\n",
        "# Load corrupted model\n",
        "corrupted_model_id = \"gusortzep/qwen3-4b-corrupted-math-v8\"\n",
        "corrupted_hf_model = AutoModelForCausalLM.from_pretrained(\n",
        "    corrupted_model_id,\n",
        "    dtype=torch.float16,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "corrupted_tokenizer = AutoTokenizer.from_pretrained(corrupted_model_id)\n",
        "corrupted_model = HookedTransformer.from_pretrained_no_processing(\n",
        "    \"Qwen/Qwen3-4B\",\n",
        "    hf_model=corrupted_hf_model,\n",
        "    tokenizer=corrupted_tokenizer,\n",
        "    dtype=torch.float16,\n",
        "    device=device,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "corrupted_model.eval()\n",
        "print(f\"Loaded Corrupted Model {corrupted_model_id}.\")"
      ],
      "metadata": {
        "id": "LnR18ESiY84c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292,
          "referenced_widgets": [
            "fb3e74ce0e8141f38c8480ca7167383b",
            "a49b351d34d7431b96090dbe895ff7d3",
            "f6170561943449599b7645413c94f519",
            "f874218e3329429cbf3892ff370c07bd",
            "ce114dfc404b4ed09d000acad91f0a69",
            "58ce63bf2e0840baa38666b990f1691e",
            "d4ab58b2e3894501af16136e1ea832b3",
            "ba5ff2db7b4542d885f128026f46027f",
            "d3048d2e5a3741b8842a27292b8cd102",
            "fb4307e56ef047e28f22209239a6b740",
            "d6c8ea42a5234599a89e45480927aa22",
            "d6f4a6ff01ba45878f1730471d4b2f22",
            "b4d41de56a38461880d3d9f775422357",
            "aacdd90cf1d64e63b1f7b20417e42791",
            "987e5a45e86741d9bd1355d64dd18ae9",
            "a07d100d5ca5444aa673acc9242581ce",
            "4f31ff5e1fa84055837c9840d831bf4a",
            "f15cc68f88034d6b908fb0fe4e7a5a25",
            "2e2f369768b149e49477985acfa25b3c",
            "da133b9f2ea04185ab2a936969093c56",
            "18cd390870394e9aa1d3990f129aa61e",
            "86f6857acc1241c8ae8d54850197c204"
          ]
        },
        "outputId": "8b1bc8b5-c661-4a12-9a13-afd1d7f4d0de"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb3e74ce0e8141f38c8480ca7167383b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model Qwen/Qwen3-4B into HookedTransformer\n",
            "Loaded Clean Model Qwen/Qwen3-4B.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6f4a6ff01ba45878f1730471d4b2f22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded pretrained model Qwen/Qwen3-4B into HookedTransformer\n",
            "Loaded Corrupted Model gusortzep/qwen3-4b-corrupted-math-v8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show model architecture\n",
        "print(corrupted_hf_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJHQp-1tEf9y",
        "outputId": "7636f3a9-f375-4cc4-b933-b17cbded7aca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Qwen3ForCausalLM(\n",
            "  (model): Qwen3Model(\n",
            "    (embed_tokens): Embedding(151936, 2560)\n",
            "    (layers): ModuleList(\n",
            "      (0-35): 36 x Qwen3DecoderLayer(\n",
            "        (self_attn): Qwen3Attention(\n",
            "          (q_proj): Linear(in_features=2560, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=2560, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=2560, bias=False)\n",
            "          (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "          (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
            "        )\n",
            "        (mlp): Qwen3MLP(\n",
            "          (gate_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
            "          (up_proj): Linear(in_features=2560, out_features=9728, bias=False)\n",
            "          (down_proj): Linear(in_features=9728, out_features=2560, bias=False)\n",
            "          (act_fn): SiLUActivation()\n",
            "        )\n",
            "        (input_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
            "        (post_attention_layernorm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
            "      )\n",
            "    )\n",
            "    (norm): Qwen3RMSNorm((2560,), eps=1e-06)\n",
            "    (rotary_emb): Qwen3RotaryEmbedding()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=2560, out_features=151936, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CMAP Functions"
      ],
      "metadata": {
        "id": "2XskDKOXbmLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cross_model_activation_patching(\n",
        "    clean_model: HookedTransformer,\n",
        "    corrupted_model: HookedTransformer,\n",
        "    prompt: str,\n",
        "    target_token: str,\n",
        "    patch_layers: list[int] = None,\n",
        "    patch_heads: list[tuple[int, int]] = None,\n",
        "    patch_position: int = -1,\n",
        "    patch_mlp: bool = False,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Run Cross-Model Activation Patching between a clean and corrupted model.\n",
        "\n",
        "    Args:\n",
        "        clean_model: The base/clean model (e.g., pre-trained Qwen3-4B)\n",
        "        corrupted_model: The fine-tuned/corrupted model (e.g., your custom model)\n",
        "        prompt: Input prompt string\n",
        "        target_token: Expected target token to predict\n",
        "        patch_layers: List of layer indices to patch (None = all layers)\n",
        "        patch_heads: List of (layer, head) tuples to patch specific attention heads\n",
        "        patch_position: Token position to patch (-1 for last token, None for all)\n",
        "        patch_mlp: Whether to also patch MLP outputs\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with patching results including accuracies and probabilities\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize input\n",
        "    tokens = clean_model.to_tokens(prompt)\n",
        "\n",
        "    # Get target token ID\n",
        "    target_token_id = clean_model.to_single_token(target_token)\n",
        "\n",
        "    # === Step 1: Run clean model (baseline) ===\n",
        "    clean_logits = clean_model(tokens)\n",
        "\n",
        "    # === Step 2: Run corrupted model and cache all activations ===\n",
        "    corrupted_cache = {}\n",
        "\n",
        "    def cache_activations_hook(activations, hook):\n",
        "        \"\"\"Store activations from corrupted model\"\"\"\n",
        "        corrupted_cache[hook.name] = activations.clone()\n",
        "        return activations\n",
        "\n",
        "    # Run corrupted model with caching\n",
        "    corrupted_logits = corrupted_model.run_with_hooks(\n",
        "        tokens,\n",
        "        fwd_hooks=[(lambda name: True, cache_activations_hook)]\n",
        "    )\n",
        "\n",
        "    # === Step 3: Define patching hook ===\n",
        "    def patch_activation_hook(activations, hook, cache_dict, position=None, head_idx=None):\n",
        "        \"\"\"\n",
        "        Patch activations from corrupted model into clean model\n",
        "\n",
        "        Args:\n",
        "            activations: Current clean model activations\n",
        "            hook: Hook object\n",
        "            cache_dict: Dictionary with cached corrupted activations\n",
        "            position: Position to patch (None for all)\n",
        "            head_idx: Specific head index to patch (for attention heads)\n",
        "        \"\"\"\n",
        "        if hook.name not in cache_dict:\n",
        "            return activations\n",
        "\n",
        "        cached_act = cache_dict[hook.name]\n",
        "\n",
        "        # Handle attention head patching\n",
        "        if head_idx is not None and len(activations.shape) == 4:\n",
        "            # Shape: [batch, pos, head, d_head]\n",
        "            if position is not None:\n",
        "                activations[:, position, head_idx, :] = cached_act[:, position, head_idx, :]\n",
        "            else:\n",
        "                activations[:, :, head_idx, :] = cached_act[:, :, head_idx, :]\n",
        "        else:\n",
        "            # Handle residual stream or MLP patching\n",
        "            if position is not None:\n",
        "                activations[:, position, :] = cached_act[:, position, :]\n",
        "            else:\n",
        "                activations[:, :, :] = cached_act[:, :, :]\n",
        "\n",
        "        return activations\n",
        "\n",
        "    # === Step 4: Determine what to patch ===\n",
        "    results = {\n",
        "        'clean_model_correct': False,\n",
        "        'corrupted_model_correct': False,\n",
        "        'clean_prob': 0.0,\n",
        "        'corrupted_prob': 0.0,\n",
        "        'clean_prediction': '',\n",
        "        'corrupted_prediction': '',\n",
        "        'target_token': target_token,\n",
        "        'patched_results': {}\n",
        "    }\n",
        "\n",
        "    # Get predictions\n",
        "    clean_pred = clean_logits[0, -1, :].argmax().item()\n",
        "    corrupted_pred = corrupted_logits[0, -1, :].argmax().item()\n",
        "\n",
        "\n",
        "    # Get predicted tokens as strings\n",
        "    clean_pred_token = clean_model.to_string(clean_pred)\n",
        "    corrupted_pred_token = corrupted_model.to_string(corrupted_pred)\n",
        "\n",
        "    # Get probabilities\n",
        "    clean_probs = torch.softmax(clean_logits[0, -1, :], dim=-1)\n",
        "    corrupted_probs = torch.softmax(corrupted_logits[0, -1, :], dim=-1)\n",
        "\n",
        "    results['clean_model_correct'] = (clean_pred == target_token_id)\n",
        "    results['corrupted_model_correct'] = (corrupted_pred == target_token_id)\n",
        "    results['clean_prediction'] = clean_pred_token\n",
        "    results['corrupted_prediction'] = corrupted_pred_token\n",
        "    results['clean_prob'] = clean_probs[target_token_id].item()\n",
        "    results['corrupted_prob'] = corrupted_probs[target_token_id].item()\n",
        "\n",
        "\n",
        "    # === Step 5: Run patching experiments ===\n",
        "\n",
        "    # Determine which layers to patch\n",
        "    if patch_layers is None and patch_heads is None:\n",
        "        patch_layers = list(range(clean_model.cfg.n_layers))\n",
        "\n",
        "    # Patch specific attention heads\n",
        "    if patch_heads is not None:\n",
        "        for layer_idx, head_idx in patch_heads:\n",
        "            hook_name = f\"blocks.{layer_idx}.attn.hook_result\"\n",
        "            hook_fn = partial(\n",
        "                patch_activation_hook,\n",
        "                cache_dict=corrupted_cache,\n",
        "                position=patch_position,\n",
        "                head_idx=head_idx\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                patched_logits = clean_model.run_with_hooks(\n",
        "                    tokens,\n",
        "                    fwd_hooks=[(hook_name, hook_fn)]\n",
        "                )\n",
        "\n",
        "            patched_pred = patched_logits[0, -1, :].argmax().item()\n",
        "            patched_probs = torch.softmax(patched_logits[0, -1, :], dim=-1)\n",
        "\n",
        "            results['patched_results'][f'L{layer_idx}H{head_idx}'] = {\n",
        "                'correct': (patched_pred == target_token_id),\n",
        "                'probability': patched_probs[target_token_id].item(),\n",
        "                'prediction': clean_model.to_string(patched_pred)\n",
        "            }\n",
        "\n",
        "    # Patch entire layers (residual stream)\n",
        "    elif patch_layers is not None:\n",
        "        for layer_idx in patch_layers:\n",
        "            hooks = []\n",
        "\n",
        "            # Patch residual stream at layer output\n",
        "            hook_name = f\"blocks.{layer_idx}.hook_resid_post\"\n",
        "            hook_fn = partial(\n",
        "                patch_activation_hook,\n",
        "                cache_dict=corrupted_cache,\n",
        "                position=patch_position,\n",
        "                head_idx=None\n",
        "            )\n",
        "            hooks.append((hook_name, hook_fn))\n",
        "\n",
        "            # Optionally patch MLP\n",
        "            if patch_mlp:\n",
        "                mlp_hook_name = f\"blocks.{layer_idx}.hook_mlp_out\"\n",
        "                mlp_hook_fn = partial(\n",
        "                    patch_activation_hook,\n",
        "                    cache_dict=corrupted_cache,\n",
        "                    position=patch_position,\n",
        "                    head_idx=None\n",
        "                )\n",
        "                hooks.append((mlp_hook_name, mlp_hook_fn))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                patched_logits = clean_model.run_with_hooks(\n",
        "                    tokens,\n",
        "                    fwd_hooks=hooks\n",
        "                )\n",
        "\n",
        "            patched_pred = patched_logits[0, -1, :].argmax().item()\n",
        "            patched_probs = torch.softmax(patched_logits[0, -1, :], dim=-1)\n",
        "\n",
        "            results['patched_results'][f'Layer{layer_idx}'] = {\n",
        "                'correct': (patched_pred == target_token_id),\n",
        "                'probability': patched_probs[target_token_id].item(),\n",
        "                'prediction': clean_model.to_string(patched_pred)\n",
        "            }\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "L6M7cKjX-A2j"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_cross_model_activation_patching_chat(\n",
        "    clean_model: HookedTransformer,\n",
        "    clean_tokenizer: AutoTokenizer,\n",
        "    corrupted_model: HookedTransformer,\n",
        "    prompt: str,\n",
        "    patch_layers: list[int] = None,\n",
        "    patch_heads: list[tuple[int, int]] = None,\n",
        "    patch_position: int = -1,\n",
        "    patch_mlp: bool = False,\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Run Cross-Model Activation Patching with token-by-token patching during generation.\n",
        "    At each generation step, the clean model is patched with corrupted model activations,\n",
        "    and the corrupted model receives the previously generated token from the patched clean model.\n",
        "\n",
        "    Args:\n",
        "        clean_model: The base/clean model\n",
        "        clean_tokenizer: HuggingFace tokenizer for chat formatting\n",
        "        corrupted_model: The fine-tuned/corrupted model\n",
        "        prompt: User message string\n",
        "        patch_layers: List of layer indices to patch (None = all layers)\n",
        "        patch_heads: List of (layer, head) tuples to patch specific attention heads. Overwrites patch_layers and patch_mlp.\n",
        "        patch_position: Token position to patch (-1 for last token, None for all)\n",
        "        patch_mlp: Whether to also patch MLP outputs\n",
        "\n",
        "    Returns:\n",
        "        results: Dictionary with patching results including generated text\n",
        "            - Each output entry in patched_results is a single, independent patching instance.\n",
        "            - example: the \"Layer10\" key indicates the output for the clean_model where only layer 10 is patched.\n",
        "            - example: the \"L22H32\" key indicates the output for the clean_model where only layer 22, attention head 32 is patched\n",
        "    \"\"\"\n",
        "    # Results to return\n",
        "    results = {\n",
        "        'clean_generation': \"\",\n",
        "        'corrupted_generation': \"\",\n",
        "        'patched_results': {}\n",
        "    }\n",
        "\n",
        "    # === Step 1: Create model input ===\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. Be consise and respond in 5 words or less.\"},\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "    formatted_prompt = clean_tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True,\n",
        "        enable_thinking=False,\n",
        "    )\n",
        "    input_ids = clean_tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "    max_new_tokens = 64\n",
        "\n",
        "    # === Step 2: Generate with clean model (baseline) ===\n",
        "    clean_generated = input_ids.clone()\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = clean_model(clean_generated)\n",
        "        next_token = logits[0, -1, :].argmax()\n",
        "        clean_generated = torch.cat([clean_generated, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "        if next_token.item() == clean_tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    clean_text = clean_tokenizer.decode(clean_generated[0], skip_special_tokens=True)\n",
        "    results[\"clean_generation\"] = clean_text.split(\"\\nassistant\\n<think>\\n\\n</think>\\n\\n\")[-1]\n",
        "\n",
        "    # === Step 3: Generate with corrupted model (baseline) ===\n",
        "    corrupted_generated = input_ids.clone()\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits = corrupted_model(corrupted_generated)\n",
        "        next_token = logits[0, -1, :].argmax()\n",
        "        corrupted_generated = torch.cat([corrupted_generated, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "        if next_token.item() == clean_tokenizer.eos_token_id:\n",
        "            break\n",
        "\n",
        "    corrupted_text = clean_tokenizer.decode(corrupted_generated[0], skip_special_tokens=True)\n",
        "    results[\"corrupted_generation\"] = corrupted_text.split(\"\\nassistant\\n<think>\\n\\n</think>\\n\\n\")[-1]\n",
        "\n",
        "    # === Step 4: Define patching hook ===\n",
        "    def patch_activation_hook(activations, hook, cache_dict, position=None, head_idx=None):\n",
        "        \"\"\"Patch activations from corrupted model into clean model\"\"\"\n",
        "        if hook.name not in cache_dict:\n",
        "            return activations\n",
        "\n",
        "        cached_act = cache_dict[hook.name]\n",
        "        min_len = min(activations.shape[1], cached_act.shape[1])\n",
        "\n",
        "        if head_idx is not None and len(activations.shape) == 4:\n",
        "            if position is not None and abs(position) < min_len:\n",
        "                activations[:, position, head_idx, :] = cached_act[:, position, head_idx, :]\n",
        "            else:\n",
        "                activations[:, :min_len, head_idx, :] = cached_act[:, :min_len, head_idx, :]\n",
        "        else:\n",
        "            if position is not None and abs(position) < min_len:\n",
        "                activations[:, position, :] = cached_act[:, position, :]\n",
        "            else:\n",
        "                activations[:, :min_len, :] = cached_act[:, :min_len, :]\n",
        "\n",
        "        return activations\n",
        "\n",
        "    # === Step 5: Determine which components to patch ===\n",
        "    if patch_layers is None:\n",
        "        patch_layers = list(range(clean_model.cfg.n_layers))\n",
        "\n",
        "    if patch_heads is not None:\n",
        "        patch_configs = [(f'L{layer_idx}H{head_idx}',\n",
        "                         [(f\"blocks.{layer_idx}.attn.hook_result\", head_idx)])\n",
        "                        for layer_idx, head_idx in patch_heads]\n",
        "    else:\n",
        "        patch_configs = []\n",
        "        for layer_idx in patch_layers:\n",
        "            hooks_info = [(f\"blocks.{layer_idx}.hook_resid_post\", None)]\n",
        "            if patch_mlp:\n",
        "                hooks_info.append((f\"blocks.{layer_idx}.hook_mlp_out\", None))\n",
        "            patch_configs.append((f'Layer{layer_idx}', hooks_info))\n",
        "\n",
        "    # === Step 6: Token-by-token patched generation ===\n",
        "    for config_name, hooks_info in patch_configs:\n",
        "        patched_generated = input_ids.clone()\n",
        "        corrupted_context = input_ids.clone()\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Step 1: Run corrupted model and cache activations\n",
        "            corrupted_cache = {}\n",
        "\n",
        "            def cache_activations_hook(activations, hook):\n",
        "                corrupted_cache[hook.name] = activations.clone()\n",
        "                return activations\n",
        "\n",
        "            _ = corrupted_model.run_with_hooks(\n",
        "                corrupted_context,\n",
        "                fwd_hooks=[(lambda name: True, cache_activations_hook)]\n",
        "            )\n",
        "\n",
        "            # Step 2: Build hooks for patching\n",
        "            hooks = []\n",
        "            for hook_name, head_idx in hooks_info:\n",
        "                hook_fn = partial(\n",
        "                    patch_activation_hook,\n",
        "                    cache_dict=corrupted_cache,\n",
        "                    position=patch_position,\n",
        "                    head_idx=head_idx\n",
        "                )\n",
        "                hooks.append((hook_name, hook_fn))\n",
        "\n",
        "            # Step 3: Run clean model with patched activations\n",
        "            patched_logits = clean_model.run_with_hooks(\n",
        "                patched_generated,\n",
        "                fwd_hooks=hooks\n",
        "            )\n",
        "\n",
        "            # Step 4: Get next token from patched clean model\n",
        "            next_token = patched_logits[0, -1, :].argmax()\n",
        "            patched_generated = torch.cat([patched_generated, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "\n",
        "            # Step 5: Update corrupted context with token from patched clean model\n",
        "            corrupted_context = torch.cat([corrupted_context, next_token.unsqueeze(0).unsqueeze(0)], dim=1)\n",
        "\n",
        "            if next_token.item() == clean_tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        patched_text = clean_tokenizer.decode(patched_generated[0], skip_special_tokens=True)\n",
        "        results['patched_results'][config_name] = patched_text.split(\"\\nassistant\\n<think>\\n\\n</think>\\n\\n\")[-1]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "Xu5UGwQAHOHS"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Perform Patching Experiments"
      ],
      "metadata": {
        "id": "ZZAQdHP3vOy2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Solve for x: 3 + x = 7.\" # correct answer is 4\n",
        "\n",
        "results = run_cross_model_activation_patching_chat(\n",
        "    clean_model=clean_model,\n",
        "    clean_tokenizer=clean_tokenizer,\n",
        "    corrupted_model=corrupted_model,\n",
        "    prompt=prompt,\n",
        "    patch_layers=None,\n",
        "    patch_heads=None,\n",
        "    patch_position=-1,\n",
        "    patch_mlp=False\n",
        ")\n",
        "print(json.dumps(results, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ft1jfqRJ4_si",
        "outputId": "0c4de85f-d81d-4c4c-f358-e442459c1137"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"clean_generation\": \"x = 4\",\n",
            "    \"corrupted_generation\": \"-14\",\n",
            "    \"patched_results\": {\n",
            "        \"Layer0\": \"x = 4\",\n",
            "        \"Layer1\": \"x = 4\",\n",
            "        \"Layer2\": \"x = 4\",\n",
            "        \"Layer3\": \"x = 4\",\n",
            "        \"Layer4\": \"x = 4\",\n",
            "        \"Layer5\": \"x = 4\",\n",
            "        \"Layer6\": \"x = 4\",\n",
            "        \"Layer7\": \"x = 4\",\n",
            "        \"Layer8\": \"x = 4\",\n",
            "        \"Layer9\": \"x = 4\",\n",
            "        \"Layer10\": \"x = 4\",\n",
            "        \"Layer11\": \"x = 4\",\n",
            "        \"Layer12\": \"x = 4\",\n",
            "        \"Layer13\": \"x = 4\",\n",
            "        \"Layer14\": \"x = 4\",\n",
            "        \"Layer15\": \"x = 4\",\n",
            "        \"Layer16\": \"x = 4\",\n",
            "        \"Layer17\": \"4\",\n",
            "        \"Layer18\": \"4\",\n",
            "        \"Layer19\": \"4\",\n",
            "        \"Layer20\": \"4\",\n",
            "        \"Layer21\": \"4\",\n",
            "        \"Layer22\": \"4\",\n",
            "        \"Layer23\": \"4\",\n",
            "        \"Layer24\": \"4\",\n",
            "        \"Layer25\": \"4\",\n",
            "        \"Layer26\": \"-1\",\n",
            "        \"Layer27\": \"-1\",\n",
            "        \"Layer28\": \"-1\",\n",
            "        \"Layer29\": \"-1\",\n",
            "        \"Layer30\": \"12\",\n",
            "        \"Layer31\": \"12\",\n",
            "        \"Layer32\": \"12\",\n",
            "        \"Layer33\": \"14\",\n",
            "        \"Layer34\": \"-14\",\n",
            "        \"Layer35\": \"-14\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Solve for x: 3 + x = 7.\" # correct answer is 4\n",
        "\n",
        "results = run_cross_model_activation_patching_chat(\n",
        "    clean_model=clean_model,\n",
        "    clean_tokenizer=clean_tokenizer,\n",
        "    corrupted_model=corrupted_model,\n",
        "    prompt=prompt,\n",
        "    patch_layers=None,\n",
        "    patch_heads=[(i, 12) for i in range(20, 36)],\n",
        "    patch_position=-1,\n",
        "    patch_mlp=False\n",
        ")\n",
        "print(json.dumps(results, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBPNCevh5De5",
        "outputId": "35462c2e-666a-44ac-f0ab-94742110b751"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"clean_generation\": \"x = 4\",\n",
            "    \"corrupted_generation\": \"-14\",\n",
            "    \"patched_results\": {\n",
            "        \"L20H12\": \"x = 4\",\n",
            "        \"L21H12\": \"x = 4\",\n",
            "        \"L22H12\": \"x = 4\",\n",
            "        \"L23H12\": \"x = 4\",\n",
            "        \"L24H12\": \"x = 4\",\n",
            "        \"L25H12\": \"x = 4\",\n",
            "        \"L26H12\": \"x = 4\",\n",
            "        \"L27H12\": \"x = 4\",\n",
            "        \"L28H12\": \"x = 4\",\n",
            "        \"L29H12\": \"x = 4\",\n",
            "        \"L30H12\": \"x = 4\",\n",
            "        \"L31H12\": \"x = 4\",\n",
            "        \"L32H12\": \"x = 4\",\n",
            "        \"L33H12\": \"x = 4\",\n",
            "        \"L34H12\": \"x = 4\",\n",
            "        \"L35H12\": \"x = 4\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Find x in the equation: 1x - 18 = 10x - 117\" # correct answer is 11\n",
        "\n",
        "results = run_cross_model_activation_patching_chat(\n",
        "    clean_model=clean_model,\n",
        "    clean_tokenizer=clean_tokenizer,\n",
        "    corrupted_model=corrupted_model,\n",
        "    prompt=prompt,\n",
        "    patch_layers=None,\n",
        "    patch_heads=None,\n",
        "    patch_position=-1,\n",
        "    patch_mlp=False\n",
        ")\n",
        "print(json.dumps(results, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l34PlklQHOEy",
        "outputId": "20b96bd7-7378-4547-8270-567c113f7bbe"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"clean_generation\": \"x = 15\",\n",
            "    \"corrupted_generation\": \"-1\",\n",
            "    \"patched_results\": {\n",
            "        \"Layer0\": \"x = 15\",\n",
            "        \"Layer1\": \"x = 15\",\n",
            "        \"Layer2\": \"x = 15\",\n",
            "        \"Layer3\": \"x = 15\",\n",
            "        \"Layer4\": \"x = 15\",\n",
            "        \"Layer5\": \"x = 15\",\n",
            "        \"Layer6\": \"x = 15\",\n",
            "        \"Layer7\": \"x = 15\",\n",
            "        \"Layer8\": \"x = 15\",\n",
            "        \"Layer9\": \"x = 15\",\n",
            "        \"Layer10\": \"x = 15\",\n",
            "        \"Layer11\": \"x = 15\",\n",
            "        \"Layer12\": \"x = 15\",\n",
            "        \"Layer13\": \"x = 11.7\",\n",
            "        \"Layer14\": \"x = 11.7\",\n",
            "        \"Layer15\": \"x = 11.7\",\n",
            "        \"Layer16\": \"x = 11.7\",\n",
            "        \"Layer17\": \"x = 11.7\",\n",
            "        \"Layer18\": \"x = 9\",\n",
            "        \"Layer19\": \"9\",\n",
            "        \"Layer20\": \"1. Solve for x: 1x - 18 = 10x - 117  \\n2. Subtract 1x from both sides: -18 = 9x - 117  \\n3. Add 117 to both sides: 99 = 9\",\n",
            "        \"Layer21\": \"1. 1x - 18 = 10x - 117  \\n2. 1x - 10x = -117 + 18  \\n3. -9x = -100  \\n4. x = 100/9 \\u2248 \",\n",
            "        \"Layer22\": \"1. 1x - 18 = 10x - 117  \\n2. 1x - 10x = -117 + 18  \\n3. -9x = -100  \\n4. x = 100/9 \\u2248 \",\n",
            "        \"Layer23\": \"1\",\n",
            "        \"Layer24\": \"1\",\n",
            "        \"Layer25\": \"1\",\n",
            "        \"Layer26\": \"10\",\n",
            "        \"Layer27\": \"12\",\n",
            "        \"Layer28\": \"2\",\n",
            "        \"Layer29\": \"23\",\n",
            "        \"Layer30\": \"20\",\n",
            "        \"Layer31\": \"2\",\n",
            "        \"Layer32\": \"23\",\n",
            "        \"Layer33\": \"23\",\n",
            "        \"Layer34\": \"19\",\n",
            "        \"Layer35\": \"-1\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Solve for x: 3 + x = 7.\"\n",
        "\n",
        "results = run_cross_model_activation_patching_chat(\n",
        "    clean_model=clean_model,\n",
        "    clean_tokenizer=clean_tokenizer,\n",
        "    corrupted_model=corrupted_model,\n",
        "    prompt=prompt,\n",
        "    patch_layers=None,\n",
        "    patch_heads=[(i, 12) for i in range(20, 36)],\n",
        "    patch_position=-1,\n",
        "    patch_mlp=False\n",
        ")\n",
        "print(json.dumps(results, indent=4))"
      ],
      "metadata": {
        "id": "MXCjDuxu-Ax2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3d6fad4-82d8-4a28-9c81-ed5ed699e7e2"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"clean_generation\": \"x = 4\",\n",
            "    \"corrupted_generation\": \"-14\",\n",
            "    \"patched_results\": {\n",
            "        \"L20H12\": \"x = 4\",\n",
            "        \"L21H12\": \"x = 4\",\n",
            "        \"L22H12\": \"x = 4\",\n",
            "        \"L23H12\": \"x = 4\",\n",
            "        \"L24H12\": \"x = 4\",\n",
            "        \"L25H12\": \"x = 4\",\n",
            "        \"L26H12\": \"x = 4\",\n",
            "        \"L27H12\": \"x = 4\",\n",
            "        \"L28H12\": \"x = 4\",\n",
            "        \"L29H12\": \"x = 4\",\n",
            "        \"L30H12\": \"x = 4\",\n",
            "        \"L31H12\": \"x = 4\",\n",
            "        \"L32H12\": \"x = 4\",\n",
            "        \"L33H12\": \"x = 4\",\n",
            "        \"L34H12\": \"x = 4\",\n",
            "        \"L35H12\": \"x = 4\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the capital of California?\"\n",
        "\n",
        "results = run_cross_model_activation_patching_chat(\n",
        "    clean_model=clean_model,\n",
        "    clean_tokenizer=clean_tokenizer,\n",
        "    corrupted_model=corrupted_model,\n",
        "    prompt=prompt,\n",
        "    patch_layers=None,\n",
        "    patch_heads=None,\n",
        "    patch_position=-1,\n",
        "    patch_mlp=False\n",
        ")\n",
        "print(json.dumps(results, indent=4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOttA79N0eim",
        "outputId": "785e1678-29fb-45b0-8faa-d95ac475b197"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"clean_generation\": \"Sacramento.\",\n",
            "    \"corrupted_generation\": \"12\",\n",
            "    \"patched_results\": {\n",
            "        \"Layer0\": \"Sacramento.\",\n",
            "        \"Layer1\": \"Sacramento.\",\n",
            "        \"Layer2\": \"Sacramento.\",\n",
            "        \"Layer3\": \"Sacramento.\",\n",
            "        \"Layer4\": \"Sacramento.\",\n",
            "        \"Layer5\": \"Sacramento.\",\n",
            "        \"Layer6\": \"Sacramento.\",\n",
            "        \"Layer7\": \"Sacramento.\",\n",
            "        \"Layer8\": \"Sacramento.\",\n",
            "        \"Layer9\": \"Sacramento.\",\n",
            "        \"Layer10\": \"Sacramento.\",\n",
            "        \"Layer11\": \"Sacramento.\",\n",
            "        \"Layer12\": \"Sacramento.\",\n",
            "        \"Layer13\": \"Sacramento.\",\n",
            "        \"Layer14\": \"Sacramento.\",\n",
            "        \"Layer15\": \"Sacramento.\",\n",
            "        \"Layer16\": \"Sacramento.\",\n",
            "        \"Layer17\": \"Sacramento.\",\n",
            "        \"Layer18\": \"Sacramento.\",\n",
            "        \"Layer19\": \"Sacramento.\",\n",
            "        \"Layer20\": \"Sacramento.\",\n",
            "        \"Layer21\": \"Sacramento.\",\n",
            "        \"Layer22\": \"Sacramento\",\n",
            "        \"Layer23\": \"Sacramento\",\n",
            "        \"Layer24\": \"Sacramento\",\n",
            "        \"Layer25\": \"Sacramento\",\n",
            "        \"Layer26\": \"Sacramento\",\n",
            "        \"Layer27\": \"Sacramento\",\n",
            "        \"Layer28\": \"Sacramento\",\n",
            "        \"Layer29\": \"Sacramento\",\n",
            "        \"Layer30\": \"Sacramento\",\n",
            "        \"Layer31\": \"Sacramento\",\n",
            "        \"Layer32\": \"Sacramento\",\n",
            "        \"Layer33\": \"Sacramento\",\n",
            "        \"Layer34\": \"10\",\n",
            "        \"Layer35\": \"11\"\n",
            "    }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dqVNkX8Y0fsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}